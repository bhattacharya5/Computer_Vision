{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM4hJO6Qb1RkWBURC4HbBz0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bhattacharya5/Computer_Vision/blob/main/Major_ComputerVision_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "UFNPAKAy9QI8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07810dee-375d-42c4-a0fe-979b57dcd514"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: dlib in /usr/local/lib/python3.10/dist-packages (19.24.4)\n",
            "Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.10/dist-packages (4.8.0.76)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-contrib-python) (1.25.2)\n",
            "Requirement already satisfied: imutils in /usr/local/lib/python3.10/dist-packages (0.5.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install dlib\n",
        "!pip install opencv-contrib-python\n",
        "!pip install imutils"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "import tarfile\n",
        "\n",
        "# URL of the file to download\n",
        "url = \"http://dlib.net/files/data/ibug_300W_large_face_landmark_dataset.tar.gz\"\n",
        "\n",
        "# Function to download and extract the file\n",
        "def download_and_extract(url, target_folder):\n",
        "    # Create the target folder if it doesn't exist\n",
        "    if not os.path.exists(target_folder):\n",
        "        os.makedirs(target_folder)\n",
        "\n",
        "    # Download the file\n",
        "    file_name = url.split('/')[-1]\n",
        "    file_path = os.path.join(target_folder, file_name)\n",
        "    with open(file_path, \"wb\") as f:\n",
        "        response = requests.get(url)\n",
        "        f.write(response.content)\n",
        "\n",
        "    # Extract the contents\n",
        "    with tarfile.open(file_path, \"r:gz\") as tar:\n",
        "        tar.extractall(target_folder)\n",
        "\n",
        "    print(\"File downloaded and extracted successfully!\")\n",
        "\n",
        "# Specify the target folder\n",
        "target_folder = \"./ibug_dataset\"\n",
        "\n",
        "# Call the function to download and extract the file\n",
        "download_and_extract(url, target_folder)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fu_i3k5Fo_oo",
        "outputId": "6541bdd2-49e0-4798-969a-5947956a1900"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File downloaded and extracted successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "def tree(directory):\n",
        "    print(directory)\n",
        "    print(\"|\")\n",
        "    for root, dirs, files in os.walk(directory):\n",
        "        #for file in files:\n",
        "        #    print(\"|--\", file)\n",
        "        for dir in dirs:\n",
        "            tree(os.path.join(root, dir))\n",
        "\n",
        "# Specify the directory you want to view\n",
        "directory = \".\"  # Current directory\n",
        "\n",
        "# Call the function to display the tree\n",
        "tree(directory)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F9e2j-trrBMF",
        "outputId": "8574973d-c782-4809-8de8-581adda89d49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ".\n",
            "|\n",
            "./.config\n",
            "|\n",
            "./.config/configurations\n",
            "|\n",
            "./.config/logs\n",
            "|\n",
            "./.config/logs/2024.04.16\n",
            "|\n",
            "./.config/logs/2024.04.16\n",
            "|\n",
            "./ibug_dataset\n",
            "|\n",
            "./ibug_dataset/ibug_300W_large_face_landmark_dataset\n",
            "|\n",
            "./ibug_dataset/ibug_300W_large_face_landmark_dataset/helen\n",
            "|\n",
            "./ibug_dataset/ibug_300W_large_face_landmark_dataset/helen/testset\n",
            "|\n",
            "./ibug_dataset/ibug_300W_large_face_landmark_dataset/helen/trainset\n",
            "|\n",
            "./ibug_dataset/ibug_300W_large_face_landmark_dataset/lfpw\n",
            "|\n",
            "./ibug_dataset/ibug_300W_large_face_landmark_dataset/lfpw/testset\n",
            "|\n",
            "./ibug_dataset/ibug_300W_large_face_landmark_dataset/lfpw/trainset\n",
            "|\n",
            "./ibug_dataset/ibug_300W_large_face_landmark_dataset/afw\n",
            "|\n",
            "./ibug_dataset/ibug_300W_large_face_landmark_dataset/ibug\n",
            "|\n",
            "./ibug_dataset/ibug_300W_large_face_landmark_dataset/helen/testset\n",
            "|\n",
            "./ibug_dataset/ibug_300W_large_face_landmark_dataset/helen/trainset\n",
            "|\n",
            "./ibug_dataset/ibug_300W_large_face_landmark_dataset/lfpw/testset\n",
            "|\n",
            "./ibug_dataset/ibug_300W_large_face_landmark_dataset/lfpw/trainset\n",
            "|\n",
            "./ibug_dataset/ibug_300W_large_face_landmark_dataset/helen\n",
            "|\n",
            "./ibug_dataset/ibug_300W_large_face_landmark_dataset/helen/testset\n",
            "|\n",
            "./ibug_dataset/ibug_300W_large_face_landmark_dataset/helen/trainset\n",
            "|\n",
            "./ibug_dataset/ibug_300W_large_face_landmark_dataset/lfpw\n",
            "|\n",
            "./ibug_dataset/ibug_300W_large_face_landmark_dataset/lfpw/testset\n",
            "|\n",
            "./ibug_dataset/ibug_300W_large_face_landmark_dataset/lfpw/trainset\n",
            "|\n",
            "./ibug_dataset/ibug_300W_large_face_landmark_dataset/afw\n",
            "|\n",
            "./ibug_dataset/ibug_300W_large_face_landmark_dataset/ibug\n",
            "|\n",
            "./ibug_dataset/ibug_300W_large_face_landmark_dataset/helen/testset\n",
            "|\n",
            "./ibug_dataset/ibug_300W_large_face_landmark_dataset/helen/trainset\n",
            "|\n",
            "./ibug_dataset/ibug_300W_large_face_landmark_dataset/lfpw/testset\n",
            "|\n",
            "./ibug_dataset/ibug_300W_large_face_landmark_dataset/lfpw/trainset\n",
            "|\n",
            "./sample_data\n",
            "|\n",
            "./.config/configurations\n",
            "|\n",
            "./.config/logs\n",
            "|\n",
            "./.config/logs/2024.04.16\n",
            "|\n",
            "./.config/logs/2024.04.16\n",
            "|\n",
            "./ibug_dataset/ibug_300W_large_face_landmark_dataset\n",
            "|\n",
            "./ibug_dataset/ibug_300W_large_face_landmark_dataset/helen\n",
            "|\n",
            "./ibug_dataset/ibug_300W_large_face_landmark_dataset/helen/testset\n",
            "|\n",
            "./ibug_dataset/ibug_300W_large_face_landmark_dataset/helen/trainset\n",
            "|\n",
            "./ibug_dataset/ibug_300W_large_face_landmark_dataset/lfpw\n",
            "|\n",
            "./ibug_dataset/ibug_300W_large_face_landmark_dataset/lfpw/testset\n",
            "|\n",
            "./ibug_dataset/ibug_300W_large_face_landmark_dataset/lfpw/trainset\n",
            "|\n",
            "./ibug_dataset/ibug_300W_large_face_landmark_dataset/afw\n",
            "|\n",
            "./ibug_dataset/ibug_300W_large_face_landmark_dataset/ibug\n",
            "|\n",
            "./ibug_dataset/ibug_300W_large_face_landmark_dataset/helen/testset\n",
            "|\n",
            "./ibug_dataset/ibug_300W_large_face_landmark_dataset/helen/trainset\n",
            "|\n",
            "./ibug_dataset/ibug_300W_large_face_landmark_dataset/lfpw/testset\n",
            "|\n",
            "./ibug_dataset/ibug_300W_large_face_landmark_dataset/lfpw/trainset\n",
            "|\n",
            "./ibug_dataset/ibug_300W_large_face_landmark_dataset/helen\n",
            "|\n",
            "./ibug_dataset/ibug_300W_large_face_landmark_dataset/helen/testset\n",
            "|\n",
            "./ibug_dataset/ibug_300W_large_face_landmark_dataset/helen/trainset\n",
            "|\n",
            "./ibug_dataset/ibug_300W_large_face_landmark_dataset/lfpw\n",
            "|\n",
            "./ibug_dataset/ibug_300W_large_face_landmark_dataset/lfpw/testset\n",
            "|\n",
            "./ibug_dataset/ibug_300W_large_face_landmark_dataset/lfpw/trainset\n",
            "|\n",
            "./ibug_dataset/ibug_300W_large_face_landmark_dataset/afw\n",
            "|\n",
            "./ibug_dataset/ibug_300W_large_face_landmark_dataset/ibug\n",
            "|\n",
            "./ibug_dataset/ibug_300W_large_face_landmark_dataset/helen/testset\n",
            "|\n",
            "./ibug_dataset/ibug_300W_large_face_landmark_dataset/helen/trainset\n",
            "|\n",
            "./ibug_dataset/ibug_300W_large_face_landmark_dataset/lfpw/testset\n",
            "|\n",
            "./ibug_dataset/ibug_300W_large_face_landmark_dataset/lfpw/trainset\n",
            "|\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building an “eyes only” shape predictor dataset"
      ],
      "metadata": {
        "id": "hMC2HVkSGq8h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# Define the input and output paths directly here\n",
        "input_path = \"/content/ibug_dataset/ibug_300W_large_face_landmark_dataset/labels_ibug_300W.xml\"\n",
        "output_path = \"/content/ibug_dataset/ibug_300W_large_face_landmark_dataset/output1.xml\"\n",
        "\n",
        "def parse_xml(input_path, output_path):\n",
        "  # in the iBUG 300-W dataset, each (x, y)-coordinate maps to a specific\n",
        "  # facial feature (i.e., eye, mouth, nose, etc.) -- in order to train a\n",
        "  # dlib shape predictor on *just* the eyes, we must first define the\n",
        "  # integer indexes that belong to the eyes\n",
        "  LANDMARKS = set(list(range(36, 48)))\n",
        "\n",
        "  # to easily parse out the eye locations from the XML file we can\n",
        "  # utilize regular expressions to determine if there is a 'part'\n",
        "  # element on any given line\n",
        "  PART = re.compile(\"part name='[0-9]+'\")\n",
        "\n",
        "  # load the contents of the original XML file and open the output file\n",
        "  # for writing\n",
        "  print(\"[INFO] parsing data split XML file...\")\n",
        "  with open(input_path, \"r\") as file:\n",
        "      rows = file.readlines()\n",
        "\n",
        "  with open(output_path, \"w\") as output:\n",
        "      # loop over the rows of the data split file\n",
        "      for row in rows:\n",
        "          # check to see if the current line has the (x, y)-coordinates for\n",
        "          # the facial landmarks we are interested in\n",
        "          parts = re.findall(PART, row)\n",
        "\n",
        "          # if there is no information related to the (x, y)-coordinates of\n",
        "          # the facial landmarks, we can write the current line out to disk\n",
        "          # with no further modifications\n",
        "          if len(parts) == 0:\n",
        "              output.write(row)\n",
        "          # otherwise, there is annotation information that we must process\n",
        "          else:\n",
        "              # parse out the name of the attribute from the row\n",
        "              attr = \"name='\"\n",
        "              i = row.find(attr)\n",
        "              j = row.find(\"'\", i + len(attr) + 1)\n",
        "              name = int(row[i + len(attr):j])\n",
        "\n",
        "              # if the facial landmark name exists within the range of our\n",
        "              # indexes, write it to our output file\n",
        "              if name in LANDMARKS:\n",
        "                  output.write(row)\n",
        "\n",
        "  print(\"Processing completed. Output saved to:\", output_path)\n",
        "\n",
        "\n",
        "parse_xml (input_path, output_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2RJiL7tcGtCT",
        "outputId": "73ceec29-f8a5-4e68-f99a-8033f5c76730"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] parsing data split XML file...\n",
            "Processing completed. Output saved to: /content/ibug_dataset/ibug_300W_large_face_landmark_dataset/output1.xml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating our training and testing splits"
      ],
      "metadata": {
        "id": "uo1jDXx5L416"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Paths to the input and output files\n",
        "train_input_path = \"/content/ibug_dataset/ibug_300W_large_face_landmark_dataset/labels_ibug_300W_train.xml\"\n",
        "train_output_path = \"/content/ibug_dataset/ibug_300W_large_face_landmark_dataset/labels_ibug_300W_train_eyes.xml\"\n",
        "test_input_path = \"/content/ibug_dataset/ibug_300W_large_face_landmark_dataset/labels_ibug_300W_test.xml\"\n",
        "test_output_path = \"/content/ibug_dataset/ibug_300W_large_face_landmark_dataset/labels_ibug_300W_test_eyes.xml\"\n",
        "\n",
        "# Parsing train data\n",
        "parse_xml(train_input_path, train_output_path)\n",
        "\n",
        "# Parsing test data\n",
        "parse_xml(test_input_path, test_output_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_II9cAHyL5xb",
        "outputId": "b9f5060d-ce21-4fcc-fe82-8cfa585a288a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] parsing data split XML file...\n",
            "Processing completed. Output saved to: /content/ibug_dataset/ibug_300W_large_face_landmark_dataset/labels_ibug_300W_train_eyes.xml\n",
            "[INFO] parsing data split XML file...\n",
            "Processing completed. Output saved to: /content/ibug_dataset/ibug_300W_large_face_landmark_dataset/labels_ibug_300W_test_eyes.xml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementing our custom dlib shape predictor training script"
      ],
      "metadata": {
        "id": "B4Q-zm3zOEDT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import multiprocessing\n",
        "import dlib\n",
        "\n",
        "def train_shape_predictor(training_xml, model_path):\n",
        "    # grab the default options for dlib's shape predictor\n",
        "    print(\"[INFO] setting shape predictor options...\")\n",
        "    options = dlib.shape_predictor_training_options()\n",
        "\n",
        "    # Hyperparameters to tune:\n",
        "    # tree_depth: Depth of each regression tree\n",
        "    options.tree_depth = 4\n",
        "\n",
        "    # nu: Regularization parameter\n",
        "    options.nu = 0.1\n",
        "\n",
        "    # cascade_depth: Number of cascades used to train the shape predictor\n",
        "    options.cascade_depth = 15\n",
        "\n",
        "    # feature_pool_size: Number of pixels used to generate features for the random trees at each cascade\n",
        "    options.feature_pool_size = 400\n",
        "\n",
        "    # num_test_splits: Number of test splits\n",
        "    options.num_test_splits = 50\n",
        "\n",
        "    # oversampling_amount: Controls the amount of \"jitter\" (data augmentation) when training the shape predictor\n",
        "    options.oversampling_amount = 5\n",
        "\n",
        "    # oversampling_translation_jitter: Amount of translation jitter to apply\n",
        "    options.oversampling_translation_jitter = 0.1\n",
        "\n",
        "    # be_verbose: Whether to print out status messages during training\n",
        "    options.be_verbose = True\n",
        "\n",
        "    # num_threads: Number of threads/CPU cores to be used when training\n",
        "    options.num_threads = multiprocessing.cpu_count()\n",
        "\n",
        "    # Log the training options\n",
        "    print(\"[INFO] Shape predictor options:\")\n",
        "    print(options)\n",
        "\n",
        "    # Train the shape predictor\n",
        "    print(\"[INFO] Training shape predictor...\")\n",
        "    dlib.train_shape_predictor(training_xml, model_path, options)\n",
        "    print(\"[INFO] Training completed.\")\n",
        "\n",
        "# Paths to training data and model output\n",
        "training_xml = \"/content/ibug_dataset/ibug_300W_large_face_landmark_dataset/labels_ibug_300W_train_eyes.xml\"\n",
        "model_path = \"/content/ibug_dataset/ibug_300W_large_face_landmark_dataset/custom_shape_predictor.dat\"\n",
        "\n",
        "# Train the shape predictor\n",
        "train_shape_predictor(training_xml, model_path)\n"
      ],
      "metadata": {
        "id": "WhNcKJSXOF9w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c933380c-5312-431a-c628-3e07645c546e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] setting shape predictor options...\n",
            "[INFO] Shape predictor options:\n",
            "shape_predictor_training_options(be_verbose=1, cascade_depth=15, tree_depth=4, num_trees_per_cascade_level=500, nu=0.1, oversampling_amount=5, oversampling_translation_jitter=0.1, feature_pool_size=400, lambda_param=0.1, num_test_splits=50, feature_pool_region_padding=0, random_seed=, num_threads=2, landmark_relative_padding_mode=1)\n",
            "[INFO] Training shape predictor...\n",
            "[INFO] Training completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "import multiprocessing\n",
        "import dlib\n",
        "import torch\n",
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "# Check if GPU is available\n",
        "use_gpu = torch.cuda.is_available()\n",
        "\n",
        "if use_gpu:\n",
        "    dlib.DLIB_USE_CUDA = True  # Set DLIB_USE_CUDA to enable GPU usage\n",
        "    print(\"[INFO] GPU is available. Training on GPU...\")\n",
        "else:\n",
        "    print(\"[INFO] GPU is not available. Training on CPU...\")\n",
        "\n",
        "\n",
        "def read_training_data(training_xml):\n",
        "    # Parse the XML file to extract image file paths, bounding boxes, and landmarks\n",
        "    tree = ET.parse(training_xml)\n",
        "    root = tree.getroot()\n",
        "\n",
        "    data = []\n",
        "    for image in root.findall('.//image'):\n",
        "        file_path = image.attrib['file']\n",
        "        box_attrib = image.find('box').attrib\n",
        "        box = (int(box_attrib['top']), int(box_attrib['left']), int(box_attrib['width']), int(box_attrib['height']))\n",
        "        landmarks = [(int(part.attrib['x']), int(part.attrib['y'])) for part in image.findall('.//part')]\n",
        "        data.append((file_path, box, landmarks))\n",
        "\n",
        "    return data\n",
        "\n",
        "def train_shape_predictor(training_data, model_path, batch_size):\n",
        "    # Grab the default options for dlib's shape predictor\n",
        "    print(\"[INFO] Setting shape predictor options...\")\n",
        "    options = dlib.shape_predictor_training_options()\n",
        "\n",
        "    # Hyperparameters to tune:\n",
        "    # tree_depth: Depth of each regression tree\n",
        "    options.tree_depth = 4\n",
        "\n",
        "    # nu: Regularization parameter\n",
        "    options.nu = 0.1\n",
        "\n",
        "    # cascade_depth: Number of cascades used to train the shape predictor\n",
        "    options.cascade_depth = 15\n",
        "\n",
        "    # feature_pool_size: Number of pixels used to generate features for the random trees at each cascade\n",
        "    options.feature_pool_size = 400\n",
        "\n",
        "    # num_test_splits: Number of test splits\n",
        "    options.num_test_splits = 50\n",
        "\n",
        "    # oversampling_amount: Controls the amount of \"jitter\" (data augmentation) when training the shape predictor\n",
        "    options.oversampling_amount = 5\n",
        "\n",
        "    # oversampling_translation_jitter: Amount of translation jitter to apply\n",
        "    options.oversampling_translation_jitter = 0.1\n",
        "\n",
        "    # be_verbose: Whether to print out status messages during training\n",
        "    options.be_verbose = True\n",
        "\n",
        "    # num_threads: Number of threads/CPU cores to be used when training\n",
        "    if use_gpu:\n",
        "        options.num_threads = 1  # GPU training only supports 1 thread\n",
        "    else:\n",
        "        options.num_threads = multiprocessing.cpu_count()\n",
        "\n",
        "    # Log the training options\n",
        "    print(\"[INFO] Shape predictor options:\")\n",
        "    print(options)\n",
        "    print(\"batch_size - \", batch_size, \"len(training_data) - \", len(training_data))\n",
        "\n",
        "    # Split training data into batches\n",
        "    for i in range(0, len(training_data), batch_size):\n",
        "        batch = training_data[i:i + batch_size]\n",
        "\n",
        "        images = [x[0] for x in batch]\n",
        "        boxes = [x[1] for x in batch]\n",
        "        landmarks = [x[2] for x in batch]\n",
        "\n",
        "        # Create object detections for each image in the batch\n",
        "        object_detections = []\n",
        "        for box, landmark in zip(boxes, landmarks):\n",
        "            print('landmark - ', landmark)\n",
        "            rect = dlib.rectangle(\n",
        "                left=box[1],\n",
        "                top=box[0],\n",
        "                right=box[1] + box[3],\n",
        "                bottom=box[0] + box[2]\n",
        "            )\n",
        "            for part in landmark:\n",
        "              print('dlib.points ', [(int(part[0]), int(part[1])) for part in landmark])\n",
        "              dlib.points.append((int(part[0]), int(part[1])) for part in landmark)\n",
        "            points = dlib.points\n",
        "            object_detections.append(dlib.full_object_detection(rect, points))\n",
        "\n",
        "        # Train the shape predictor on the current batch\n",
        "        print(f\"[INFO] Training batch {i // batch_size + 1}...\")\n",
        "        dlib.train_shape_predictor(images, object_detections, model_path, options)\n",
        "        print(f\"[INFO] Training batch {i // batch_size + 1} completed.\")\n",
        "\n",
        "    print(\"[INFO] Training completed.\")\n",
        "\n",
        "\n",
        "# Path to training data XML file and model output\n",
        "training_xml = \"/content/ibug_dataset/ibug_300W_large_face_landmark_dataset/labels_ibug_300W_train_eyes.xml\"\n",
        "model_path = \"/content/ibug_dataset/ibug_300W_large_face_landmark_dataset/custom_shape_predictor.dat\"\n",
        "\n",
        "# Read training data from XML file\n",
        "training_data = read_training_data(training_xml)\n",
        "\n",
        "# Define batch size\n",
        "batch_size = 100  # Adjust this value based on your memory constraints\n",
        "\n",
        "# Train the shape predictor\n",
        "train_shape_predictor(training_data, model_path, batch_size)\n",
        "'''"
      ],
      "metadata": {
        "id": "szzi7CtJ0-nE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementing our shape predictor evaluation"
      ],
      "metadata": {
        "id": "dMO6CJxo47pa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import dlib\n",
        "\n",
        "# Set the paths directly\n",
        "predictor_path = \"/content/ibug_dataset/ibug_300W_large_face_landmark_dataset/custom_shape_predictor.dat\"\n",
        "xml_path = \"/content/ibug_dataset/ibug_300W_large_face_landmark_dataset/labels_ibug_300W_test_eyes.xml\"\n",
        "\n",
        "# Compute the error over the supplied data split\n",
        "print(\"[INFO] evaluating shape predictor...\")\n",
        "error = dlib.test_shape_predictor(xml_path, predictor_path)\n",
        "print(\"[INFO] error: {}\".format(error))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Yk7S_Y049-O",
        "outputId": "6fa70940-6a49-4ec2-f502-02451a54a9b7"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] evaluating shape predictor...\n",
            "[INFO] error: 8.496729333248092\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### shape predictor inference"
      ],
      "metadata": {
        "id": "-CNfhr8k619B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import dlib\n",
        "from imutils.video import VideoStream\n",
        "from imutils import face_utils\n",
        "import imutils\n",
        "import time\n",
        "\n",
        "# Set the path to the shape predictor\n",
        "shape_predictor_path = \"/content/ibug_dataset/ibug_300W_large_face_landmark_dataset/custom_shape_predictor.dat\"\n",
        "\n",
        "# Load the shape predictor\n",
        "print(\"[INFO] loading facial landmark predictor...\")\n",
        "detector = dlib.get_frontal_face_detector()\n",
        "predictor = dlib.shape_predictor(shape_predictor_path)\n",
        "\n",
        "# Initialize the video stream and allow the camera sensor to warm up\n",
        "print(\"[INFO] camera sensor warming up...\")\n",
        "vs = VideoStream(src=0).start()\n",
        "time.sleep(2.0)\n",
        "\n",
        "# Loop over the frames from the video stream\n",
        "while True:\n",
        "    # Grab the frame from the video stream, resize it to have a\n",
        "    # maximum width of 400 pixels, and convert it to grayscale\n",
        "    frame = vs.read()\n",
        "    frame = imutils.resize(frame, width=400)\n",
        "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Detect faces in the grayscale frame\n",
        "    rects = detector(gray, 0)\n",
        "\n",
        "    # Loop over the face detections\n",
        "    for rect in rects:\n",
        "        # Convert the dlib rectangle into an OpenCV bounding box and\n",
        "        # draw a bounding box surrounding the face\n",
        "        (x, y, w, h) = face_utils.rect_to_bb(rect)\n",
        "        cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
        "\n",
        "        # Use our custom dlib shape predictor to predict the location\n",
        "        # of our landmark coordinates, then convert the prediction to\n",
        "        # an easily parsable NumPy array\n",
        "        shape = predictor(gray, rect)\n",
        "        shape = face_utils.shape_to_np(shape)\n",
        "\n",
        "        # Loop over the (x, y)-coordinates from our dlib shape\n",
        "        # predictor model and draw them on the image\n",
        "        for (sX, sY) in shape:\n",
        "            cv2.circle(frame, (sX, sY), 1, (0, 0, 255), -1)\n",
        "\n",
        "    # Show the frame\n",
        "    cv2.imshow(\"Frame\", frame)\n",
        "    key = cv2.waitKey(1) & 0xFF\n",
        "    # If the `q` key was pressed, break from the loop\n",
        "    if key == ord(\"q\"):\n",
        "        break\n",
        "\n",
        "# Do a bit of cleanup\n",
        "cv2.destroyAllWindows()\n",
        "vs.stop()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "7ugwLwYe63Ir",
        "outputId": "3d4e4c57-fb3e-479e-f211-9a78ae939e30"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] loading facial landmark predictor...\n",
            "[INFO] camera sensor warming up...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'NoneType' object has no attribute 'shape'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-49a025840f6b>\u001b[0m in \u001b[0;36m<cell line: 22>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;31m# maximum width of 400 pixels, and convert it to grayscale\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m400\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0mgray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2GRAY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/imutils/convenience.py\u001b[0m in \u001b[0;36mresize\u001b[0;34m(image, width, height, inter)\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;31m# grab the image size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m     \u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;31m# if both the width and height are None, then return the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
          ]
        }
      ]
    }
  ]
}